import streamlit as st
import pandas as pd
import os
import time
import json
import tempfile
from processor import UniversalProcessor
from gsheets import upload_to_gsheet, find_file_in_folder, create_spreadsheet_in_folder, get_service_account_quota, read_all_sheets_data, update_supplier_sheet
from rapidfuzz import process, fuzz, utils

st.set_page_config(page_title="Excel Document Processor", layout="wide")

APP_VERSION = "1.1.18"

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã Google Drive
SYSTEMS_FOLDER_ID = "1ijv4no6aI3E_le5zAe12QGssehrUETkh"
SUPPLIERS_FOLDER_ID = "1BroJAZivTEypJjFsAOU6uODMaeyw2K7n"
SUPPLIER_TEMPLATE_ID = "1hXBDqliS5rYgL3ZUe2Cg5PUnLAs7aPCwDn8Icivn9qc"

# –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞ –¢–£
TU_MAPPING_FILE = "/Users/aleksandrprokudin/Documents/check/searchTU/–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –†–£, –¢–£ –∏ –¢–®–ü –ø–æ —Ç–æ—á–∫–∞–º.xlsx"

# –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –Ω–∞—Å—Ç—Ä–æ–µ–∫
SETTINGS_FILE = "settings.json"

@st.cache_data
def load_tu_mapping(file_path):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫ –¢–£ –∏–∑ Excel.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–≤–∞ —Å–ª–æ–≤–∞—Ä—è:
    syrye_map: { "ADDRESS_PART": "FIO" } - –¥–ª—è "–°—ã—Ä—å–µ /"
    regular_map: { "FULL_NAME": "FIO" } - –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö
    """
    if not os.path.exists(file_path):
        return {}, {}
    
    try:
        # –õ–∏—Å—Ç "–¢–æ—á–∫–∞-–¢–£ –°–ü" (–¥–ª—è –°—ã—Ä—å—è)
        # –ò—â–µ–º –≤ col index 3 (D), –±–µ—Ä–µ–º col index 18 (S)
        df_sp = pd.read_excel(file_path, sheet_name="–¢–æ—á–∫–∞-–¢–£ –°–ü", header=None)
        syrye_map = {}
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏ (—à–∞–ø–∫—É), –±–µ—Ä–µ–º –¥–∞–Ω–Ω—ã–µ
        for idx, row in df_sp.iterrows():
            if idx < 2: continue # Skip header rows
            key = str(row[3]).strip()
            val = str(row[18]).strip()
            if key and key.lower() != "nan" and val and val.lower() != "nan":
                syrye_map[key] = val
                
        # –õ–∏—Å—Ç "–¢–æ—á–∫–∞-–¢–£" (–¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö)
        # –ò—â–µ–º –≤ col index 2 (C), –±–µ—Ä–µ–º col index 12 (M)
        df_reg = pd.read_excel(file_path, sheet_name="–¢–æ—á–∫–∞-–¢–£", header=None)
        regular_map = {}
        for idx, row in df_reg.iterrows():
            if idx < 2: continue
            key = str(row[2]).strip()
            val = str(row[12]).strip()
            if key and key.lower() != "nan" and val and val.lower() != "nan":
                regular_map[key] = val
                
        return syrye_map, regular_map
    except Exception as e:
        print(f"[ERROR] Loading TU Mapping: {e}")
        return {}, {}

def find_tu_for_warehouse(warehouse_name, syrye_map, regular_map):
    """
    –ò—â–µ—Ç –¢–£ –¥–ª—è —Å–∫–ª–∞–¥–∞ IIKO.
    """
    if not warehouse_name:
        return ""
        
    w_name = str(warehouse_name).strip()
    
    # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –°—ã—Ä—å–µ
    if w_name.lower().startswith("—Å—ã—Ä—å–µ"):
        # "–°—ã—Ä—å–µ / –ö–†–î –ö—Ä–∞—Å–Ω–∞—è —É–ª., 176" -> "–ö–†–î –ö—Ä–∞—Å–Ω–∞—è —É–ª., 176"
        parts = w_name.split("/", 1)
        if len(parts) > 1:
            target = parts[1].strip()
            # Fuzzy match against syrye_map keys
            match = process.extractOne(target, syrye_map.keys(), scorer=fuzz.token_set_ratio)
            if match and match[1] >= 85:
                return syrye_map[match[0]]
    
    # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –û–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫ (–∏–ª–∏ –µ—Å–ª–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è 1 –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∞)
    # –ò—â–µ–º –ø–æ–ª–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –≤ regular_map
    match = process.extractOne(w_name, regular_map.keys(), scorer=fuzz.token_set_ratio)
    if match and match[1] >= 85:
        return regular_map[match[0]]
        
    return ""

def load_settings():
    defaults = {
        "income_k": "–ø–ª–∞—Ç–µ–∂–Ω–æ–µ, –ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏–µ, –æ–ø–ª–∞—Ç–∞, —Å–ø–∏—Å–∞–Ω–∏–µ, –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–æ, –ø—Ä–∏—Ö–æ–¥",
        "expense_k": "—Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è, —É–ø–¥, –ø—Ä–æ–¥–∞–∂–∞, –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞, –∞–∫—Ç",
        "target_month": "–Ø–Ω–≤–∞—Ä—å 26",
        "suppliers": {} # { "Supplier Name": "Spreadsheet ID" }
    }
    if os.path.exists(SETTINGS_FILE):
        try:
            with open(SETTINGS_FILE, "r", encoding="utf-8") as f:
                saved = json.load(f)
                # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –∫–ª—é—á–∏ –ø–∞–ø–æ–∫ –∏ —à–∞–±–ª–æ–Ω–∞, –µ—Å–ª–∏ –æ–Ω–∏ –±—ã–ª–∏ –≤ JSON
                for k in ["systems_folder_id", "suppliers_folder_id", "supplier_template_id"]:
                    saved.pop(k, None)
                return {**defaults, **saved}
        except:
            return defaults
    return defaults

def save_settings(income_k, expense_k, target_month, suppliers):
    with open(SETTINGS_FILE, "w", encoding="utf-8") as f:
        json.dump({
            "income_k": income_k, 
            "expense_k": expense_k,
            "target_month": target_month,
            "suppliers": suppliers
        }, f, ensure_ascii=False)

settings = load_settings()

st.title("üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞–∫—Ç–æ–≤ —Å–≤–µ—Ä–∫–∏")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≤ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏
with st.sidebar:
    st.header("üìÖ –ü–µ—Ä–∏–æ–¥")
    months = ["–Ø–Ω–≤–∞—Ä—å", "–§–µ–≤—Ä–∞–ª—å", "–ú–∞—Ä—Ç", "–ê–ø—Ä–µ–ª—å", "–ú–∞–π", "–ò—é–Ω—å", "–ò—é–ª—å", "–ê–≤–≥—É—Å—Ç", "–°–µ–Ω—Ç—è–±—Ä—å", "–û–∫—Ç—è–±—Ä—å", "–ù–æ—è–±—Ä—å", "–î–µ–∫–∞–±—Ä—å"]
    years = ["25", "26", "27"]
    
    # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å —Ç–µ–∫—É—â–∏–π –º–µ—Å—è—Ü –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫
    try:
        cur_m, cur_y = settings["target_month"].split(" ")
        m_idx = months.index(cur_m)
        y_idx = years.index(cur_y)
    except:
        m_idx, y_idx = 0, 1

    sel_m = st.selectbox("–ú–µ—Å—è—Ü", months, index=m_idx)
    sel_y = st.selectbox("–ì–æ–¥", years, index=y_idx)
    target_month = f"{sel_m} {sel_y}"

    # –ò–ò –≤—Å–µ–≥–¥–∞ Yandex, –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–∫—Ä—ã—Ç—ã
    model_name = "yandexgpt"
    # –ö–ª—é—á–∏ –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –∏–∑ .env
    api_key = os.getenv("YANDEX_API_KEY", "")
    folder_id = os.getenv("YANDEX_FOLDER_ID", "")

    st.divider()
    st.header("üè¢ –°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ –ü–æ—Å—Ç–∞–≤—â–∏–∫–æ–≤")
    new_supplier_name = st.text_input("–ù–∞–∑–≤–∞–Ω–∏–µ –ø–æ—Å—Ç–∞–≤—â–∏–∫–∞")
    new_supplier_id = st.text_input("ID —Ç–∞–±–ª–∏—Ü—ã (–æ—Å—Ç–∞–≤—å—Ç–µ –ø—É—Å—Ç—ã–º –¥–ª—è –∞–≤—Ç–æ-—Å–æ–∑–¥–∞–Ω–∏—è)")
    
    current_suppliers = settings["suppliers"].copy()
    if st.button("‚ûï –î–æ–±–∞–≤–∏—Ç—å –ø–æ—Å—Ç–∞–≤—â–∏–∫–∞"):
        if new_supplier_name:
            if not new_supplier_id:
                with st.spinner(f"–°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –¥–ª—è {new_supplier_name} –ø–æ —à–∞–±–ª–æ–Ω—É..."):
                    new_id = create_spreadsheet_in_folder(new_supplier_name, SUPPLIERS_FOLDER_ID, SUPPLIER_TEMPLATE_ID)
                    if new_id:
                        current_suppliers[new_supplier_name] = new_id
                        st.success(f"–°–æ–∑–¥–∞–Ω–∞ —Ç–∞–±–ª–∏—Ü–∞ ID: {new_id}")
                    else:
                        st.error("–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Ç–∞–±–ª–∏—Ü—ã")
            else:
                current_suppliers[new_supplier_name] = new_supplier_id
                st.success(f"–î–æ–±–∞–≤–ª–µ–Ω: {new_supplier_name}")
        else:
            st.warning("–í–≤–µ–¥–∏—Ç–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–æ—Å—Ç–∞–≤—â–∏–∫–∞")

    if current_suppliers:
        st.write("–°–ø–∏—Å–æ–∫:")
        for s_name in list(current_suppliers.keys()):
            col1, col2 = st.columns([3, 1])
            col1.text(f"{s_name}")
            if col2.button("üóëÔ∏è", key=f"del_{s_name}"):
                del current_suppliers[s_name]
                save_settings(settings["income_k"], settings["expense_k"], target_month, current_suppliers)
                st.rerun()

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –ø—Ä–∞–≤ (–§–∞–π–ª –∏–ª–∏ –°–µ–∫—Ä–µ—Ç—ã)
    has_creds = os.path.exists("credentials.json") or "gcp_service_account" in st.secrets
    if not has_creds:
        st.warning("‚ö†Ô∏è –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è Google –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞. –û—Ç–ø—Ä–∞–≤–∫–∞ –±—É–¥–µ—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞.")

    st.divider()
    st.header("üîç –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Ñ–∏–ª—å—Ç—Ä–∞")
    
    inc_k = st.text_area("–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –î–û–•–û–î–ê", value=settings["income_k"])
    exp_k = st.text_area("–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –†–ê–°–•–û–î–ê", value=settings["expense_k"])
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    if (inc_k != settings["income_k"] or 
        exp_k != settings["expense_k"] or 
        target_month != settings["target_month"] or
        current_suppliers != settings["suppliers"]):
        save_settings(inc_k, exp_k, target_month, current_suppliers)
        st.rerun()
    
    income_list = [x.strip() for x in inc_k.split(",") if x.strip()]
    expense_list = [x.strip() for x in exp_k.split(",") if x.strip()]

    st.divider()
    st.caption(f"–í–µ—Ä—Å–∏—è: {APP_VERSION}")

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–≤–æ—Ç—É —Å–µ—Ä–≤–∏—Å–Ω–æ–≥–æ –∞–∫–∫–∞—É–Ω—Ç–∞ (–¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ 403 –æ—à–∏–±–∫–∏)
    try:
        from gsheets import get_service_account_quota
        quota = get_service_account_quota()
        if quota:
            usage = int(quota.get('usage', 0)) / (1024**3)
            limit = int(quota.get('limit', 0)) / (1024**3) if 'limit' in quota else 15
            st.caption(f"Drive Storage: {usage:.2f} GB / {limit:.2f} GB used")
    except:
        pass

st.info(f"üìÖ –í—ã–±—Ä–∞–Ω –ø–µ—Ä–∏–æ–¥: **{target_month}**")

def normalize_doc_num_for_search(val):
    if not val:
        return ""
    # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä—ã –∏ –±—É–∫–≤—ã, —É–±–∏—Ä–∞–µ–º –Ω—É–ª–∏ –≤ –Ω–∞—á–∞–ª–µ
    s = str(val).strip().lower()
    s = "".join(c for c in s if c.isalnum())
    s = s.lstrip("0")
    return s

def find_doc_in_index(target_doc, idx_map):
    """
    –ü—ã—Ç–∞–µ—Ç—Å—è –Ω–∞–π—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç –≤ –∏–Ω–¥–µ–∫—Å–µ:
    1. –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
    2. –ï—Å–ª–∏ —Ç–æ—á–Ω–æ–≥–æ –Ω–µ—Ç - –∏—â–µ—Ç –≤—Ö–æ–∂–¥–µ–Ω–∏–µ (–µ—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª–∏–Ω–Ω—ã–π)
    """
    if not target_doc:
        return []
        
    # 1. Exact match
    if target_doc in idx_map:
        return idx_map[target_doc]
    
    # 2. Substring match (Fallack)
    # –û–ø–∞—Å–Ω–æ –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –Ω–æ–º–µ—Ä–æ–≤ ("20" –Ω–∞–π–¥–µ—Ç "120"), –ø–æ—ç—Ç–æ–º—É —Å—Ç–∞–≤–∏–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ
    # –ï—Å–ª–∏ –Ω–æ–º–µ—Ä —Å–æ–¥–µ—Ä–∂–∏—Ç –±—É–∫–≤—ã (20dp), —Ä–∏—Å–∫ –º–µ–Ω—å—à–µ.
    # –ï—Å–ª–∏ —Ç–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä—ã, –Ω—É–∂–Ω–∞ –¥–ª–∏–Ω–∞ —Ö–æ—Ç—è –±—ã 3-4? –ò–ª–∏ –¥–æ–≤–µ—Ä–∏–º—Å—è?
    # –ö–µ–π—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: "20" vs "20dp". "20" (len 2) in "20dp".
    # –ß—Ç–æ–±—ã –Ω–µ –∑–∞—Ü–µ–ø–∏—Ç—å "120", –ø—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ –æ–Ω–æ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å —ç—Ç–æ–≥–æ –Ω–æ–º–µ—Ä–∞
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: –Ω–µ –ø–µ—Ä–µ–±–∏—Ä–∞—Ç—å –≤–µ—Å—å —Å–ª–æ–≤–∞—Ä—å, –µ—Å–ª–∏ –æ–Ω –æ–≥—Ä–æ–º–Ω—ã–π. –ù–æ —É –Ω–∞—Å ~500-1000 –¥–æ–∫–æ–≤.
    # –ò—â–µ–º –∫–ª—é—á–∏, –∫–æ—Ç–æ—Ä—ã–µ –ù–ê–ß–ò–ù–ê–Æ–¢–°–Ø —Å target_doc (20 -> 20dp)
    # –ò–ª–∏ –∫–ª—é—á–∏, –∫–æ—Ç–æ—Ä—ã–µ —è–≤–ª—è—é—Ç—Å—è target_doc (20dp -> 20 - –≤—Ä—è–¥ –ª–∏ LLM –æ–±—Ä–µ–∂–µ—Ç –±—É–∫–≤—ã)
    
    for key in idx_map:
        # Key = System Doc (e.g. "20dp")
        # Target = Act Doc (e.g. "20")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ 1: –°–∏—Å—Ç–µ–º–∞ "20dp", –ê–∫—Ç "20". "20dp" starts with "20"
        if key.startswith(target_doc) and len(key) > len(target_doc):
             # –î–æ–ø –ø—Ä–æ–≤–µ—Ä–∫–∞: —Å–ª–µ–¥—É—é—â–∏–π —Å–∏–º–≤–æ–ª –ø–æ—Å–ª–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è - –±—É–∫–≤–∞?
             # 20dp -> 20 (ok), 205 -> 20 (bad)
             suffix = key[len(target_doc):]
             if suffix[0].isalpha(): # DP starts with D
                 return idx_map[key]
                 
    return []

def is_correction(text, amount=None):
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–ø–∏—Å—å –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–æ–π/–≤–æ–∑–≤—Ä–∞—Ç–æ–º.
    1. –ü–æ —Ç–µ–∫—Å—Ç—É (—Å–æ–¥–µ—Ä–∂–∏—Ç "–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞", "–≤–æ–∑–≤—Ä–∞—Ç")
    2. –ü–æ —Å—É–º–º–µ (–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–∞—è)
    """
    t = str(text).lower()
    if "–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞" in t or "–≤–æ–∑–≤—Ä–∞—Ç" in t:
        return True
    if amount is not None:
        try:
            if float(amount) < 0:
                return True
        except:
            pass
    return False

def perform_reconciliation(act_data, system_data_map, supplier_name):
    # Load TU Mapping
    syrye_map, regular_map = load_tu_mapping(TU_MAPPING_FILE)
    
    # act_data headers: ["–î–∞—Ç–∞", "–¢–µ–∫—Å—Ç", "–ù–æ–º–µ—Ä", "–°—É–º–º–∞"]
    # system_data_map: { "IIKO": [records...], "SBIS": [records...], ... }
    
    # 1. Prepare fast lookups for systems
    # Filter each system by supplier name (fuzzy) and index by doc number
    
    # Config for system columns
    system_cols = {
        "IIKO": {"partner": "–ü–æ—Å—Ç–∞–≤—â–∏–∫/–ü–æ–∫—É–ø–∞—Ç–µ–ª—å", "doc": "–í—Ö–æ–¥—è—â–∏–π –Ω–æ–º–µ—Ä", "sum": "–°—É–º–º–∞, —Ä.", "comment": "–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π"},
        "DOCSINBOX": {"partner": "–ü–æ—Å—Ç–∞–≤—â–∏–∫", "doc": "–ù–æ–º–µ—Ä –Ω–∞–∫–ª–∞–¥–Ω–æ–π –ø–æ—Å—Ç–∞–≤—â–∏–∫–∞", "sum": "–°—É–º–º–∞"},
        "SBIS": {"partner": "–ö–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç", "doc": "–ù–æ–º–µ—Ä", "sum": "–°—É–º–º–∞"},
        "SAP": {"partner": "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç–∞", "doc": "–°—Å—ã–ª–∫–∞", "sum": "–°—É–º–º–∞ –≤ –í–í", "docType": "–í–∏–¥ –¥–æ–∫—É–º–µ–Ω—Ç–∞"},
        "FB": {"partner": "–ü–æ—Å—Ç–∞–≤—â–∏–∫", "doc": "–ù–æ–º–µ—Ä", "sum": "–°—É–º–º–∞", "linked": "–ü—Ä–∏–≤—è–∑–∞–Ω –∫ –ø–æ—Å—Ç–∞–≤–∫–µ", "point": "–¢–æ—á–∫–∞"}
    }
    
    # Pre-process system data: filter by supplier and index by normalized doc number
    system_indices = {} 
    
    # Counters for system docs (excluding corrections)
    system_stats = {
        "IIKO": {"total_sum": 0.0, "count": 0},
        "SAP": {"total_sum": 0.0, "count": 0},
        "FB": {"total_sum": 0.0, "count": 0}
    }
    
    for sys_name, records in system_data_map.items():
        if sys_name not in system_cols:
            continue
            
        cols = system_cols[sys_name]
        
        # Fuzzy match supplier name
        clean_supplier_name = supplier_name.split("(")[0].strip()
        
        # Collect all unique supplier names from system data
        unique_partners = set()
        for r in records:
            p = r.get(cols["partner"], "")
            if p:
                unique_partners.add(str(p).strip()) 
        
        print(f"[RECON]   System {sys_name} has {len(unique_partners)} unique partners.")
             
        # Find matches using clean name and partial_ratio (best for substrings)
        matches = process.extract(
            clean_supplier_name, 
            unique_partners, 
            scorer=fuzz.partial_ratio, 
            limit=5,
            processor=utils.default_process 
        )
        
        # Filter by cutoff
        matched_partners = {m[0] for m in matches if m[1] >= 85}
        
        if matched_partners:
            print(f"[RECON]   ACCEPTED matches in {sys_name}: {list(matched_partners)}")
        else:
            print(f"[RECON]   NO matches accepted in {sys_name} (threshold 65)")
        
        # Index records and calculate stats
        idx_map = {}
        for r in records:
            p = str(r.get(cols["partner"], ""))
            if p in matched_partners:
                doc = r.get(cols["doc"])
                norm_doc = normalize_doc_num_for_search(doc)
                
                amount_str = r.get(cols["sum"], "0")
                try:
                    amount_float = float(str(amount_str).replace(",", ".").replace("\xa0", "").strip() or 0)
                except:
                    amount_float = 0.0
                
                # Check for correction (to exclude from stats)
                # Check negative amount
                # For SAP, normal amounts are negative. Don't use negative sign as correction indicator.
                check_amount = amount_float if sys_name != "SAP" else None
                is_corr = is_correction("", check_amount)
                
                # Check specific fields text
                if not is_corr and sys_name == "IIKO":
                    comment = str(r.get("–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π", "")).lower()
                    if "–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞" in comment or "–≤–æ–∑–≤—Ä–∞—Ç" in comment:
                        is_corr = True
                
                # Add to stats if NOT correction
                if not is_corr and sys_name in system_stats:
                    system_stats[sys_name]["total_sum"] += amount_float
                    system_stats[sys_name]["count"] += 1
                    
                # Store record for matching (we keep corrections in index to match against act corrections)
                if norm_doc not in idx_map:
                    idx_map[norm_doc] = []
                idx_map[norm_doc].append({"amount": amount_float, "raw": r})
                
        system_indices[sys_name] = idx_map

        # Check for duplicates in IIKO
        if sys_name == "IIKO":
            # 1. Duplicates
            dups = []
            for k, v in idx_map.items():
                if len(v) > 1:
                    # Collect original doc numbers
                    orig_doc = v[0]["raw"].get(cols["doc"], k)
                    dups.append(str(orig_doc))
            if dups:
                system_stats["IIKO"]["duplicates"] = ", ".join(dups)
            
            # 2. Missing in Act (present in IIKO but not in Act)
            # Initially, all docs are unmatched
            system_indices["IIKO_unmatched"] = set(idx_map.keys())

    # 3. Build Result Table & Act Stats
    results = []
    
    act_stats = {"total_sum": 0.0, "count": 0}
    act_missing_in_iiko = [] # Documents in Act but not in IIKO
    
    print(f"\n[RECON] Starting reconciliation for supplier: '{supplier_name}'")
    for sys_name, idx_map in system_indices.items():
        print(f"[RECON] System {sys_name}: {len(idx_map)} docs indexed for this supplier.")
    
    for row in act_data:
        # row: [Date, Text, DocNum, Amount]
        date = row[0]
        text = row[1] 
        doc_num = row[2]
        amount_act_raw = row[3]
        
        try:
            amount_act = float(str(amount_act_raw).replace(",", ".").replace("\xa0", "").strip() or 0)
        except:
            amount_act = 0.0
            
        # Check correction for Act stats
        if not is_correction(text):
            act_stats["total_sum"] += amount_act
            act_stats["count"] += 1
        
        # –û—Å–Ω–æ–≤–Ω–æ–π –±–ª–æ–∫ (–ü–æ—Å—Ç–∞–≤—â–∏–∫)
        res_row = {
            "supplier_date": date,
            "supplier_doc": text, 
            "supplier_sum": amount_act
        }
        
        norm_doc = normalize_doc_num_for_search(doc_num)
        
        # IIKO
        iiko_idx = system_indices.get("IIKO", {})
        iiko_wh_found = "" # To store warehouse for TU lookup
        
        matches = find_doc_in_index(norm_doc, iiko_idx)
        if matches:
            m = matches[0]
            raw = m["raw"]
            res_row["iiko_date"] = raw.get("–î–∞—Ç–∞", "")
            res_row["iiko_doc"] = raw.get("–í—Ö–æ–¥—è—â–∏–π –Ω–æ–º–µ—Ä", "")
            res_row["iiko_partner"] = raw.get("–ü–æ—Å—Ç–∞–≤—â–∏–∫/–ü–æ–∫—É–ø–∞—Ç–µ–ª—å", "")
            
            wh = raw.get("–°–∫–ª–∞–¥", "")
            res_row["iiko_warehouse"] = wh
            iiko_wh_found = wh
            
            res_row["iiko_sum"] = m["amount"]
            res_row["iiko_comment"] = raw.get("–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π", "")
            res_row["iiko_delta"] = amount_act - m["amount"]
            
            # Mark as found (remove from unmatched set)
            if "IIKO_unmatched" in system_indices:
                if norm_doc in system_indices["IIKO_unmatched"]:
                    system_indices["IIKO_unmatched"].discard(norm_doc)
                else:
                    # Try to find by prefix if it was a fuzzy match
                    to_remove = []
                    for k in system_indices["IIKO_unmatched"]:
                        if k.startswith(norm_doc) and len(k) > len(norm_doc):
                             to_remove.append(k)
                    for k in to_remove:
                        system_indices["IIKO_unmatched"].discard(k)
        else:
            res_row["iiko_delta"] = amount_act 
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ø–∏—Å–æ–∫ "–õ–∏—à–Ω–∏–µ –≤ –ê–∫—Ç–µ"
            # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –Ω–æ–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞
            if doc_num and str(doc_num).strip():
                act_missing_in_iiko.append(str(doc_num).strip())
        
        # FB (New System)
        fb_idx = system_indices.get("FB", {})
        matches = find_doc_in_index(norm_doc, fb_idx)
        if matches:
            m = matches[0]
            raw = m["raw"]
            res_row["fb_doc"] = raw.get("–ù–æ–º–µ—Ä", "")
            res_row["fb_type"] = raw.get("–¢–∏–ø", "")
            res_row["fb_linked"] = raw.get("–ü—Ä–∏–≤—è–∑–∞–Ω –∫ –ø–æ—Å—Ç–∞–≤–∫–µ", "")
            res_row["fb_partner"] = raw.get("–ü–æ—Å—Ç–∞–≤—â–∏–∫", "")
            res_row["fb_point"] = raw.get("–¢–æ—á–∫–∞", "")
            res_row["fb_date"] = raw.get("–î–∞—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞", "")
            res_row["fb_status"] = raw.get("–°—Ç–∞—Ç—É—Å", "")
            res_row["fb_del_status"] = raw.get("–°—Ç–∞—Ç—É—Å –ø–æ—Å—Ç–∞–≤–∫–∏", "")
            res_row["fb_sum"] = m["amount"]
            res_row["fb_delta"] = amount_act - m["amount"]
        else:
             res_row["fb_delta"] = amount_act
            
        # DOCSINBOX
        dxbx_idx = system_indices.get("DOCSINBOX", {})
        matches = find_doc_in_index(norm_doc, dxbx_idx)
        if matches:
            m = matches[0]
            raw = m["raw"]
            res_row["dxbx_buyer"] = raw.get("–ü–æ–∫—É–ø–∞—Ç–µ–ª—å", "")
            res_row["dxbx_status"] = raw.get("–°—Ç–∞—Ç—É—Å –ø—Ä–∏–µ–º–∫–∏", "")
            
            # Lookup TU based on IIKO warehouse
            if iiko_wh_found:
                tu_name = find_tu_for_warehouse(iiko_wh_found, syrye_map, regular_map)
                res_row["dxbx_tu"] = tu_name
            
            # FALLBACK: If TU not found via IIKO, try to find via DXBX Buyer
            if not res_row.get("dxbx_tu") and res_row.get("dxbx_buyer"):
                buyer_raw = res_row["dxbx_buyer"]
                # Clean: remove content in brackets and extra spaces
                buyer_clean = buyer_raw.split("(")[0].strip()
                
                # Search in regular_map (addresses) with lower threshold
                # Use token_set_ratio to handle word reordering and extra words
                match = process.extractOne(buyer_clean, regular_map.keys(), scorer=fuzz.token_set_ratio)
                if match and match[1] >= 60:
                     print(f"[RECON] TU Fallback: '{buyer_clean}' -> '{match[0]}' ({match[1]}%)")
                     res_row["dxbx_tu"] = regular_map[match[0]]
        
        # SBIS
        sbis_idx = system_indices.get("SBIS", {})
        matches = find_doc_in_index(norm_doc, sbis_idx)
        if matches:
            m = matches[0]
            raw = m["raw"]
            res_row["sbis_status"] = raw.get("–°—Ç–∞—Ç—É—Å", "")
            res_row["sbis_delta"] = amount_act - m["amount"]
        else:
            res_row["sbis_delta"] = amount_act
            
        # SAP
        sap_idx = system_indices.get("SAP", {})
        matches = find_doc_in_index(norm_doc, sap_idx)
        if matches:
            m = matches[0]
            raw = m["raw"]
            res_row["sap_doc_type"] = raw.get("–í–∏–¥ –¥–æ–∫—É–º–µ–Ω—Ç–∞", "")
            # SAP amounts are negative. Delta = Act + SAP (e.g. 100 + (-100) = 0)
            res_row["sap_delta"] = amount_act + m["amount"]
        else:
            res_row["sap_delta"] = amount_act
            
        # –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π
        res_row["manager_comment"] = ""
                
        results.append(res_row)
        
    # Collect unmatched IIKO docs names
    iiko_missing_in_act = []
    if "IIKO_unmatched" in system_indices and "IIKO" in system_indices:
        for k in system_indices["IIKO_unmatched"]:
            # Get original doc name from the first record in the list
            records = system_indices["IIKO"].get(k, [])
            if records:
                orig = records[0]["raw"].get("–í—Ö–æ–¥—è—â–∏–π –Ω–æ–º–µ—Ä", k)
                iiko_missing_in_act.append(str(orig))
                
    summary = {
        "iiko_total": system_stats["IIKO"]["total_sum"],
        "sap_total": system_stats["SAP"]["total_sum"],
        "fb_total": system_stats["FB"]["total_sum"],
        "act_total": act_stats["total_sum"],
        
        "delta_act_iiko": act_stats["total_sum"] - system_stats["IIKO"]["total_sum"],
        # SAP amounts are negative. Sum them up to get delta.
        "delta_act_sap": act_stats["total_sum"] + system_stats["SAP"]["total_sum"],
        "delta_act_fb": act_stats["total_sum"] - system_stats["FB"]["total_sum"],
        
        "act_count": act_stats["count"],
        "iiko_count": system_stats["IIKO"]["count"],
        "delta_count": act_stats["count"] - system_stats["IIKO"]["count"],
        
        "iiko_duplicates": system_stats["IIKO"].get("duplicates", ""),
        "iiko_missing": ", ".join(iiko_missing_in_act),
        "act_missing": ", ".join(act_missing_in_iiko)
    }
        
    return {"rows": results, "summary": summary}

uploaded_files = st.file_uploader("–í—ã–±–µ—Ä–∏—Ç–µ Excel —Ñ–∞–π–ª—ã", type=["xlsx", "xls"], accept_multiple_files=True)

# –û–ø—Ü–∏—è –¥–ª—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –Ω–æ–º–µ—Ä–∞
extraction_mode = st.radio(
    "Strategia –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –Ω–æ–º–µ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞:",
    ("–ê–≤—Ç–æ (–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –°/–§)", "–°—Ç—Ä–æ–≥–æ –ø–µ—Ä–≤—ã–π –Ω–æ–º–µ—Ä (–ê–∫—Ç)"),
    horizontal=True,
    help="–í—ã–±–µ—Ä–∏—Ç–µ '–ê–≤—Ç–æ', —á—Ç–æ–±—ã –∏—Å–∫–∞—Ç—å –Ω–æ–º–µ—Ä —Å—á–µ—Ç–∞-—Ñ–∞–∫—Ç—É—Ä—ã (–æ–±—ã—á–Ω–æ –≤ —Å–∫–æ–±–∫–∞—Ö). –í—ã–±–µ—Ä–∏—Ç–µ '–ê–∫—Ç', —á—Ç–æ–±—ã –±—Ä–∞—Ç—å –ø–µ—Ä–≤—ã–π –Ω–æ–º–µ—Ä (–Ω–æ–º–µ—Ä –Ω–∞–∫–ª–∞–¥–Ω–æ–π)."
)

if "results" not in st.session_state:
    st.session_state.results = {}

if uploaded_files:
    for file_index, uploaded_file in enumerate(uploaded_files):
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∂–∏–º –≤ –∫–ª—é—á, —á—Ç–æ–±—ã –ø—Ä–∏ —Å–º–µ–Ω–µ —Ä–∞–¥–∏–æ-–∫–Ω–æ–ø–∫–∏ –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–ª–æ—Å—å
        file_key = f"{uploaded_file.name}_{uploaded_file.size}_{file_index}_{extraction_mode}"
        
        if file_key not in st.session_state.results:
            with st.spinner(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {uploaded_file.name}..."):
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π tempfile –¥–ª—è –æ–±–ª–∞–∫–∞
                with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(uploaded_file.name)[1]) as tmp:
                    tmp.write(uploaded_file.getbuffer())
                    temp_path = tmp.name
                
                try:
                    processor = UniversalProcessor(model_name=model_name)
                    data, status, system_name, headers = processor.process_file(
                        temp_path,
                        income_keywords=income_list,
                        expense_keywords=expense_list,
                        extraction_mode=extraction_mode
                    )
                    
                    if status in ("enriched", "enriched_system"):
                        if not data:
                            st.warning(f"–í —Ñ–∞–π–ª–µ {uploaded_file.name} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã–≥—Ä—É–∑–∫–∏.")
                        else:
                            st.session_state.results[file_key] = {
                                "data": data,
                                "system": system_name,
                                "filename": uploaded_file.name,
                                "headers": headers
                            }
                    else:
                        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {uploaded_file.name}. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —ç—Ç–æ Excel —Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏.")
                finally:
                    if os.path.exists(temp_path):
                        os.remove(temp_path)

        if file_key in st.session_state.results:
            res_obj = st.session_state.results[file_key]
            data = res_obj["data"]
            system = res_obj["system"]
            filename = res_obj["filename"]
            headers = res_obj.get("headers", [])
            
            with st.expander(f"üìä {filename} [–°–∏—Å—Ç–µ–º–∞: {system}]", expanded=True):
                if data and headers:
                    df = pd.DataFrame(data, columns=headers).astype(str)
                    st.dataframe(df, use_container_width=True)
                    
                    if system != "OTHER":
                        if st.button(f"üöÄ –û—Ç–ø—Ä–∞–≤–∏—Ç—å –≤ –°–∏—Å—Ç–µ–º—ã ({system})", key=f"btn_{file_key}"):
                            with st.spinner("–ò—â–µ–º/—Å–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –º–µ—Å—è—Ü–∞..."):
                                gs_id = find_file_in_folder(SYSTEMS_FOLDER_ID, target_month)
                                if not gs_id:
                                    gs_id = create_spreadsheet_in_folder(target_month, SYSTEMS_FOLDER_ID)
                                
                                if gs_id:
                                    success, msg = upload_to_gsheet(gs_id, system, data, headers)
                                    if success:
                                        st.success(f"{msg} –≤ —Ç–∞–±–ª–∏—Ü—É '{target_month}'")
                                    else:
                                        st.error(msg)
                    else:
                        st.info("–≠—Ç–æ –ø–æ—Ö–æ–∂–µ –Ω–∞ –ê–∫—Ç —Å–≤–µ—Ä–∫–∏. –í—ã–±–µ—Ä–∏—Ç–µ –ø–æ—Å—Ç–∞–≤—â–∏–∫–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏:")
                        selected_supplier = st.selectbox(
                            "–í—ã–±–µ—Ä–∏—Ç–µ –ø–æ—Å—Ç–∞–≤—â–∏–∫–∞",
                            ["-- –í—ã–±–µ—Ä–∏—Ç–µ --"] + list(current_suppliers.keys()),
                            key=f"sel_{file_key}"
                        )
                        
                        if selected_supplier != "-- –í—ã–±–µ—Ä–∏—Ç–µ --":
                            supplier_id = current_suppliers[selected_supplier]
                            
                            col1, col2 = st.columns(2)
                            
                            with col1:
                                if st.button(f"üöÄ –û—Ç–ø—Ä–∞–≤–∏—Ç—å –ü–æ—Å—Ç–∞–≤—â–∏–∫—É (–ò—Å—Ö–æ–¥–Ω—ã–π)", key=f"btn_orig_{file_key}"):
                                    success, msg = upload_to_gsheet(supplier_id, target_month, data, headers)
                                    if success:
                                        st.success(f"{msg} (–ª–∏—Å—Ç {target_month})")
                                    else:
                                        st.error(msg)
                                        
                            with col2:
                                if st.button(f"‚öîÔ∏è –°—Ä–∞–≤–Ω–∏—Ç—å —Å –¥–∞–Ω–Ω—ã–º–∏ —Å–∏—Å—Ç–µ–º", key=f"btn_recon_{file_key}"):
                                    with st.spinner("–ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å–∏—Å—Ç–µ–º –¥–ª—è —Å–≤–µ—Ä–∫–∏..."):
                                        # 1. Find system spreadsheet
                                        sys_ss_id = find_file_in_folder(SYSTEMS_FOLDER_ID, target_month)
                                        if not sys_ss_id:
                                            st.error(f"–ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∑–∞ –ø–µ—Ä–∏–æ–¥ {target_month}")
                                        else:
                                            st.toast(f"–§–∞–π–ª —Å–∏—Å—Ç–µ–º –Ω–∞–π–¥–µ–Ω: {sys_ss_id}")
                                            # 2. Read all sheets
                                            sys_data = read_all_sheets_data(sys_ss_id)
                                            if not sys_data:
                                                st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –¥–∞–Ω–Ω—ã–µ —Å–∏—Å—Ç–µ–º")
                                            else:
                                                sheet_counts = {k: len(v) for k, v in sys_data.items()}
                                                st.toast(f"–î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {sheet_counts}")
                                                
                                                # 3. Perform reconciliation
                                                st.info(f"–°–≤–µ—Ä–∫–∞ –¥–ª—è –ø–æ—Å—Ç–∞–≤—â–∏–∫–∞: {selected_supplier}")
                                                recon_result_obj = perform_reconciliation(data, sys_data, selected_supplier)
                                                
                                                # Save results to session state to display
                                                st.session_state[f"recon_{file_key}"] = recon_result_obj
                                                st.toast(f"–°–≤–µ—Ä–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –ù–∞–π–¥–µ–Ω–æ {len(recon_result_obj['rows'])} —Å—Ç—Ä–æ–∫.")
                                                
                        # Display reconciliation results if available
                        if f"recon_{file_key}" in st.session_state:
                            recon_res_obj = st.session_state[f"recon_{file_key}"]
                            recon_rows = recon_res_obj["rows"]
                            summary = recon_res_obj.get("summary", {})
                            
                            st.divider()
                            st.write("### üèÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–≤–µ—Ä–∫–∏")
                            
                            # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Å–∞–º–º–∞—Ä–∏
                            c1, c2, c3, c4 = st.columns(4)
                            c1.metric("–û–±–æ—Ä–æ—Ç –ê–∫—Ç", f"{summary.get('act_total',0):,.2f}")
                            c2.metric("–û–±–æ—Ä–æ—Ç IIKO", f"{summary.get('iiko_total',0):,.2f}")
                            c3.metric("–û–±–æ—Ä–æ—Ç SAP", f"{summary.get('sap_total',0):,.2f}")
                            c4.metric("–û–±–æ—Ä–æ—Ç FB", f"{summary.get('fb_total',0):,.2f}")
                            
                            c5, c6, c7 = st.columns(3)
                            c5.metric("Œî –ê–∫—Ç-IIKO", f"{summary.get('delta_act_iiko',0):,.2f}")
                            c6.metric("Œî –ê–∫—Ç-SAP", f"{summary.get('delta_act_sap',0):,.2f}")
                            c7.metric("Œî –ê–∫—Ç-FB", f"{summary.get('delta_act_fb',0):,.2f}")
                            
                            # New metrics: document counts
                            d1, d2, d3 = st.columns(3)
                            d1.metric("–ö–æ–ª-–≤–æ –¥–æ–∫. –ê–∫—Ç", f"{summary.get('act_count',0):,.0f}")
                            d2.metric("–ö–æ–ª-–≤–æ –¥–æ–∫. IIKO", f"{summary.get('iiko_count',0):,.0f}")
                            d3.metric("Œî –ö–æ–ª-–≤–æ", f"{summary.get('delta_count',0):,.0f}")

                            if summary.get("iiko_duplicates"):
                                st.warning(f"‚ö†Ô∏è –ù–∞–π–¥–µ–Ω—ã –¥—É–±–ª–∏–∫–∞—Ç—ã –Ω–∞–∫–ª–∞–¥–Ω—ã—Ö –≤ IIKO: {summary['iiko_duplicates']}")
                            
                            if summary.get("iiko_missing"):
                                st.error(f"üö´ –ù–∞–π–¥–µ–Ω—ã –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ IIKO, –∫–æ—Ç–æ—Ä—ã—Ö –ù–ï–¢ –≤ –ê–∫—Ç–µ: {summary['iiko_missing']}")
                            
                            if summary.get("act_missing"):
                                st.error(f"‚ùì –ù–∞–π–¥–µ–Ω—ã –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –ê–∫—Ç–µ, –∫–æ—Ç–æ—Ä—ã—Ö –ù–ï–¢ –≤ IIKO: {summary['act_missing']}")
                            
                            st.dataframe(pd.DataFrame(recon_rows))
                            
                            if st.button(f"üíæ –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–≤–µ—Ä–∫–∏", key=f"btn_save_recon_{file_key}"):
                                supplier_id = current_suppliers[selected_supplier]
                                # Create a special sheet name for reconciliation
                                recon_sheet_name = f"–°–≤–µ—Ä–∫–∞ {target_month}"
                                success, msg = update_supplier_sheet(supplier_id, recon_sheet_name, recon_rows, summary)
                                if success:
                                    st.success(msg)
                                else:
                                    st.error(msg)
